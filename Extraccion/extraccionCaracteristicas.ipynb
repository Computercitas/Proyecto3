{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de caracteristicas usando SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "clS8YjQym1MI"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_sift_features(image_url):\n",
    "    \"\"\"\n",
    "    Extrae características SIFT de una imagen desde URL\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = imread(image_url) # Obtener imagen desde URL\n",
    "        if len(img.shape) == 3:\n",
    "            img = rgb2gray(img)\n",
    "\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        sift = cv2.SIFT_create()\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "\n",
    "        if descriptors is None:\n",
    "            return None, None\n",
    "\n",
    "        mean_descriptor = np.mean(descriptors, axis=0)\n",
    "        return keypoints, mean_descriptor\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando imagen: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def process_and_save_incremental(csv_file, output_dir):\n",
    "    \"\"\"\n",
    "    Procesa y guarda características de forma incremental, reanudando desde el último checkpoint\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    checkpoint_file = f\"{output_dir}/checkpoint.json\"\n",
    "    descriptors_file = f\"{output_dir}/descriptores.npy\"\n",
    "\n",
    "    # Cargar estado anterior si existe\n",
    "    processed = {}\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            processed = json.load(f)\n",
    "\n",
    "    descriptors_list = []\n",
    "    if os.path.exists(descriptors_file):\n",
    "        descriptors_list = list(np.load(descriptors_file))\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    failed_images = []\n",
    "\n",
    "    print(f\"Iniciando procesamiento desde imagen {len(processed)}\")\n",
    "\n",
    "    for idx, (filename, image_url) in enumerate(zip(df['filename'], df['link'])):\n",
    "        if filename in processed:\n",
    "            continue\n",
    "\n",
    "        print(f\"Procesando {idx+1}/{len(df)}: {filename}\")\n",
    "\n",
    "        try:\n",
    "            keypoints, descriptor = extract_sift_features(image_url)\n",
    "\n",
    "            if descriptor is not None:\n",
    "                descriptors_list.append(descriptor)\n",
    "                processed[filename] = len(descriptors_list) - 1\n",
    "\n",
    "                np.save(descriptors_file, np.vstack(descriptors_list))\n",
    "                with open(checkpoint_file, 'w') as f:\n",
    "                    json.dump(processed, f)\n",
    "\n",
    "                print(f\"Guardado - Total procesadas: {len(processed)}\")\n",
    "            else:\n",
    "                failed_images.append(filename)\n",
    "                print(f\"Fallo al procesar: {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error en {filename}: {e}\")\n",
    "            failed_images.append(filename)\n",
    "\n",
    "        # Guardar lista de fallos\n",
    "        with open(f\"{output_dir}/failed.json\", 'w') as f:\n",
    "            json.dump(failed_images, f)\n",
    "\n",
    "    print(\"\\nProcesamiento completado:\")\n",
    "    print(f\"- Total procesadas: {len(processed)}\")\n",
    "    print(f\"- Fallos: {len(failed_images)}\")\n",
    "\n",
    "    return np.vstack(descriptors_list), processed, failed_images\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hAcmrZqVu_ne"
   },
   "outputs": [],
   "source": [
    "\n",
    "# def load_features(output_dir):\n",
    "#     \"\"\"\n",
    "#     Carga descriptores y mapeo desde archivos\n",
    "#     \"\"\"\n",
    "#     descriptors = np.load(f\"{output_dir}/descriptores.npy\")\n",
    "#     with open(f\"{output_dir}/checkpoint.json\", 'r') as f:\n",
    "#         mapping = json.load(f)\n",
    "#     return descriptors, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "679tri1dm1Je"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando procesamiento desde imagen 3403\n",
      "Procesando 3404/44446: 41267.jpg\n",
      "Guardado - Total procesadas: 3404\n",
      "Procesando 3405/44446: 11517.jpg\n",
      "Guardado - Total procesadas: 3405\n",
      "Procesando 3406/44446: 25722.jpg\n",
      "Guardado - Total procesadas: 3406\n",
      "Procesando 3407/44446: 55455.jpg\n",
      "Guardado - Total procesadas: 3407\n",
      "Procesando 3408/44446: 12392.jpg\n",
      "Guardado - Total procesadas: 3408\n",
      "Procesando 3409/44446: 31110.jpg\n",
      "Guardado - Total procesadas: 3409\n",
      "Procesando 3410/44446: 2617.jpg\n",
      "Guardado - Total procesadas: 3410\n",
      "Procesando 3411/44446: 43530.jpg\n",
      "Guardado - Total procesadas: 3411\n",
      "Procesando 3412/44446: 13240.jpg\n",
      "Guardado - Total procesadas: 3412\n",
      "Procesando 3413/44446: 54587.jpg\n",
      "Guardado - Total procesadas: 3413\n",
      "Procesando 3414/44446: 27075.jpg\n",
      "Guardado - Total procesadas: 3414\n",
      "Procesando 3415/44446: 46042.jpg\n",
      "Guardado - Total procesadas: 3415\n",
      "Procesando 3416/44446: 7365.jpg\n",
      "Guardado - Total procesadas: 3416\n",
      "Procesando 3417/44446: 49836.jpg\n",
      "Guardado - Total procesadas: 3417\n",
      "Procesando 3418/44446: 16732.jpg\n",
      "Guardado - Total procesadas: 3418\n",
      "Procesando 3419/44446: 34462.jpg\n",
      "Guardado - Total procesadas: 3419\n",
      "Procesando 3420/44446: 47190.jpg\n",
      "Guardado - Total procesadas: 3420\n",
      "Procesando 3421/44446: 27849.jpg\n",
      "Guardado - Total procesadas: 3421\n",
      "Procesando 3422/44446: 52214.jpg\n",
      "Guardado - Total procesadas: 3422\n",
      "Procesando 3423/44446: 4184.jpg\n",
      "Guardado - Total procesadas: 3423\n",
      "Procesando 3424/44446: 36751.jpg\n",
      "Guardado - Total procesadas: 3424\n",
      "Procesando 3425/44446: 44371.jpg\n",
      "Guardado - Total procesadas: 3425\n",
      "Procesando 3426/44446: 37683.jpg\n",
      "Guardado - Total procesadas: 3426\n",
      "Procesando 3427/44446: 14401.jpg\n",
      "Guardado - Total procesadas: 3427\n",
      "Procesando 3428/44446: 39719.jpg\n",
      "Guardado - Total procesadas: 3428\n",
      "Procesando 3429/44446: 41603.jpg\n",
      "Guardado - Total procesadas: 3429\n",
      "Procesando 3430/44446: 11173.jpg\n",
      "Guardado - Total procesadas: 3430\n",
      "Procesando 3431/44446: 25346.jpg\n",
      "Guardado - Total procesadas: 3431\n",
      "Procesando 3432/44446: 24294.jpg\n",
      "Guardado - Total procesadas: 3432\n",
      "Procesando 3433/44446: 57766.jpg\n",
      "Guardado - Total procesadas: 3433\n",
      "Procesando 3434/44446: 2273.jpg\n",
      "Guardado - Total procesadas: 3434\n",
      "Procesando 3435/44446: 43154.jpg\n",
      "Guardado - Total procesadas: 3435\n",
      "Procesando 3436/44446: 13624.jpg\n",
      "Guardado - Total procesadas: 3436\n",
      "Procesando 3437/44446: 27411.jpg\n",
      "Guardado - Total procesadas: 3437\n",
      "Procesando 3438/44446: 55031.jpg\n",
      "Guardado - Total procesadas: 3438\n",
      "Procesando 3439/44446: 42086.jpg\n"
     ]
    }
   ],
   "source": [
    "# Uso\n",
    "output_dir = \"features\"\n",
    "descriptors, mapping, failed = process_and_save_incremental(\"images.csv\", output_dir)\n",
    "\n",
    "# Cargar cuando sea necesario\n",
    "# descriptors, mapping = load_features(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnDwlD4nn_zH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptWbZo4_m1G2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeJgqoSum0_g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYHPE6kS-2pW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
